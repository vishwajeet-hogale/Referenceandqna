{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "questionansweringdfsapi.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishwajeet-hogale/Referenceandqna/blob/main/questionansweringdfsapi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frTeTcy4WdbY"
      },
      "source": [
        "!nvidia-smi\n",
        "!pip install colabcode\n",
        "!pip install fastapi\n",
        "!git clone https://github.com/huggingface/transformers \\\n",
        "&& cd transformers \\\n",
        "&& git checkout a3085020ed0d81d4903c50967687192e3101e770 \n",
        "!pip install ./transformers\n",
        "!pip install tensorboardX\n",
        "!mkdir dataset \\\n",
        "&& cd dataset \\\n",
        "&& wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json \\\n",
        "&& wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFuvxNSnyGu_"
      },
      "source": [
        "from colabcode import ColabCode\n",
        "from fastapi import FastAPI"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp0Pq9z9Y4S0",
        "cellView": "code"
      },
      "source": [
        "# import os\n",
        "# import torch\n",
        "# import time\n",
        "# from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# from transformers import (\n",
        "#     AlbertConfig,\n",
        "#     AlbertForQuestionAnswering,\n",
        "#     AlbertTokenizer,\n",
        "#     squad_convert_examples_to_features\n",
        "# )\n",
        "\n",
        "# from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n",
        "\n",
        "# from transformers.data.metrics.squad_metrics import compute_predictions_logits\n",
        "\n",
        "# # READER NOTE: Set this flag to use own model, or use pretrained model in the Hugging Face repository\n",
        "# use_own_model = False\n",
        "\n",
        "# if use_own_model:\n",
        "#   model_name_or_path = \"/content/model_output\"\n",
        "# else:\n",
        "#   model_name_or_path = \"ktrapeznikov/albert-xlarge-v2-squad-v2\"\n",
        "\n",
        "# output_dir = \"\"\n",
        "\n",
        "# # Config\n",
        "# n_best_size = 1\n",
        "# max_answer_length = 30\n",
        "# do_lower_case = True\n",
        "# null_score_diff_threshold = 0.0\n",
        "\n",
        "# def to_list(tensor):\n",
        "#     return tensor.detach().cpu().tolist()\n",
        "\n",
        "# # Setup model\n",
        "# config_class, model_class, tokenizer_class = (\n",
        "#     AlbertConfig, AlbertForQuestionAnswering, AlbertTokenizer)\n",
        "# config = config_class.from_pretrained(model_name_or_path)\n",
        "# print(config)\n",
        "# tokenizer = tokenizer_class.from_pretrained(\n",
        "#     model_name_or_path, do_lower_case=True)\n",
        "# model = model_class.from_pretrained(model_name_or_path, config=config)\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model.to(device)\n",
        "\n",
        "# processor = SquadV2Processor()\n",
        "\n",
        "# def run_prediction(question_texts, context_text):\n",
        "#     \"\"\"Setup function to compute predictions\"\"\"\n",
        "#     examples = []\n",
        "\n",
        "#     for i, question_text in enumerate(question_texts):\n",
        "#         example = SquadExample(\n",
        "#             qas_id=str(i),\n",
        "#             question_text=question_text,\n",
        "#             context_text=context_text,\n",
        "#             answer_text=None,\n",
        "#             start_position_character=None,\n",
        "#             title=\"Predict\",\n",
        "#             is_impossible=False,\n",
        "#             answers=None,\n",
        "#         )\n",
        "\n",
        "#         examples.append(example)\n",
        "\n",
        "#     features, dataset = squad_convert_examples_to_features(\n",
        "#         examples=examples,\n",
        "#         tokenizer=tokenizer,\n",
        "#         max_seq_length=384,\n",
        "#         doc_stride=128,\n",
        "#         max_query_length=64,\n",
        "#         is_training=False,\n",
        "#         return_dataset=\"pt\",\n",
        "#         threads=1,\n",
        "#     )\n",
        "\n",
        "#     eval_sampler = SequentialSampler(dataset)\n",
        "#     eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=10)\n",
        "\n",
        "#     all_results = []\n",
        "\n",
        "#     for batch in eval_dataloader:\n",
        "#         model.eval()\n",
        "#         batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             inputs = {\n",
        "#                 \"input_ids\": batch[0],\n",
        "#                 \"attention_mask\": batch[1],\n",
        "#                 \"token_type_ids\": batch[2],\n",
        "#             }\n",
        "\n",
        "#             example_indices = batch[3]\n",
        "\n",
        "#             outputs = model(**inputs)\n",
        "#             print(outputs[0])\n",
        "#             for i, example_index in enumerate(example_indices):\n",
        "#                 eval_feature = features[example_index.item()]\n",
        "#                 unique_id = int(eval_feature.unique_id)\n",
        "\n",
        "#                 output = [to_list(output[i]) for output in outputs]\n",
        "               \n",
        "#                 # print(len(output))\n",
        "#                 start_logits, end_logits = output\n",
        "#                 result = SquadResult(unique_id, start_logits, end_logits)\n",
        "#                 all_results.append(result)\n",
        "#     # print(len(all_results))\n",
        "#     output_prediction_file = \"predictions.json\"\n",
        "#     output_nbest_file = \"nbest_predictions.json\"\n",
        "#     output_null_log_odds_file = \"null_predictions.json\"\n",
        "\n",
        "#     predictions = compute_predictions_logits(\n",
        "#         examples,\n",
        "#         features,\n",
        "#         all_results,\n",
        "#         n_best_size,\n",
        "#         max_answer_length,\n",
        "#         do_lower_case,\n",
        "#         output_prediction_file,\n",
        "#         output_nbest_file,\n",
        "#         output_null_log_odds_file,\n",
        "#         False,  # verbose_logging\n",
        "#         True,  # version_2_with_negative\n",
        "#         null_score_diff_threshold,\n",
        "#         tokenizer,\n",
        "#     )\n",
        "\n",
        "#     return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-sUrcA5nXTH",
        "cellView": "code"
      },
      "source": [
        "# #we have to read csv here!\n",
        "# context = \"Incorporated in 1999, MTAR Technologies is a leading national player in the precision engineering industry. The company is primarily engaged in the manufacturing of mission-critical precision components with close tolerance and in critical assemblies through its precision machining, assembly, specialized fabrication, testing, and quality control processes. Since its inception, MTAR Technologies has significantly expanded its product portfolio including critical assemblies i.e. Liquid propulsion engines to GSLV Mark III, Base Shroud Assembly & Airframes for Agni Programs, Actuators for LCA, power units for fuel cells, Fuel machining head, Bridge & Column, Drive Mechanisms, Thimble Package, etc. A wide range of complex product portfolios meets the varied requirements of the Indian nuclear, Defence, and Space sector. ISRO, NPCIL, DRDO, Bloom Energy, Rafael, Elbit, etc. are some of the esteem clients. Currently, the firm has 7 state-of-the-art manufacturing facilities in Hyderabad, Telangana that undertake precision machining, assembly, specialized fabrication, brazing and heat treatment, testing and quality control, and other specialized processes.\"\n",
        "# questions = [\"which company is going to be listed?\",\"which company is going to be IPO?\",\"which company is going to be public?\",\"which company is going to be listed on stock exchange?\",\"which company is about to launch initial public offering?\",\"which company is about to launch IPO?\",\"which company is it talking about?\",\"where is the company located?\",\"in which country is it?\"]\n",
        "\n",
        "# # Run method\n",
        "# predictions = run_prediction(questions, context)\n",
        "\n",
        "# # Print results\n",
        "# import statistics as st\n",
        "# prelist = [i for i in predictions.values() if i != '']\n",
        "# try:\n",
        "#   print(st.mode(prelist))\n",
        "# except:\n",
        "#   print(prelist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftrObnIs4zuo"
      },
      "source": [
        "cc = ColabCode(port=12000, code=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRNXvKe7NJ1b"
      },
      "source": [
        "from pydantic import BaseModel, conlist\n",
        "from typing import List\n",
        "\n",
        "class Qna(BaseModel):\n",
        "  question_texts : list\n",
        "  context_text : str\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEEcjHoQyZF2"
      },
      "source": [
        "app = FastAPI(title=\"ML Models as API on Google Colab\", description=\"with FastAPI and ColabCode\", version=\"1.0\")\n",
        "\n",
        "@app.post(\"/api\", tags=[\"prediction\"])\n",
        "async def get_predictions(qna: Qna):\n",
        "    try:\n",
        "        question_texts = dict(qna)[\"question_texts\"]\n",
        "        context_text = dict(qna)[\"context_text\"]\n",
        "        print(question_texts,context_text)\n",
        "        import os\n",
        "        import torch\n",
        "        import time\n",
        "        from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "        from transformers import (\n",
        "            AlbertConfig,\n",
        "            AlbertForQuestionAnswering,\n",
        "            AlbertTokenizer,\n",
        "            squad_convert_examples_to_features\n",
        "        )\n",
        "\n",
        "        from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n",
        "\n",
        "        from transformers.data.metrics.squad_metrics import compute_predictions_logits\n",
        "\n",
        "        # READER NOTE: Set this flag to use own model, or use pretrained model in the Hugging Face repository\n",
        "        use_own_model = False\n",
        "\n",
        "        if use_own_model:\n",
        "          model_name_or_path = \"/content/model_output\"\n",
        "        else:\n",
        "          model_name_or_path = \"ktrapeznikov/albert-xlarge-v2-squad-v2\"\n",
        "\n",
        "        output_dir = \"\"\n",
        "\n",
        "        # Config\n",
        "        n_best_size = 1\n",
        "        max_answer_length = 30\n",
        "        do_lower_case = True\n",
        "        null_score_diff_threshold = 0.0\n",
        "\n",
        "        def to_list(tensor):\n",
        "            return tensor.detach().cpu().tolist()\n",
        "\n",
        "        # Setup model\n",
        "        config_class, model_class, tokenizer_class = (\n",
        "            AlbertConfig, AlbertForQuestionAnswering, AlbertTokenizer)\n",
        "        config = config_class.from_pretrained(model_name_or_path)\n",
        "        print(config)\n",
        "        tokenizer = tokenizer_class.from_pretrained(\n",
        "            model_name_or_path, do_lower_case=True)\n",
        "        model = model_class.from_pretrained(model_name_or_path, config=config)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        model.to(device)\n",
        "\n",
        "        processor = SquadV2Processor()\n",
        "\n",
        "        def run_prediction(question_texts, context_text):\n",
        "            \"\"\"Setup function to compute predictions\"\"\"\n",
        "            examples = []\n",
        "\n",
        "            for i, question_text in enumerate(question_texts):\n",
        "                example = SquadExample(\n",
        "                    qas_id=str(i),\n",
        "                    question_text=question_text,\n",
        "                    context_text=context_text,\n",
        "                    answer_text=None,\n",
        "                    start_position_character=None,\n",
        "                    title=\"Predict\",\n",
        "                    is_impossible=False,\n",
        "                    answers=None,\n",
        "                )\n",
        "\n",
        "                examples.append(example)\n",
        "\n",
        "            features, dataset = squad_convert_examples_to_features(\n",
        "                examples=examples,\n",
        "                tokenizer=tokenizer,\n",
        "                max_seq_length=384,\n",
        "                doc_stride=128,\n",
        "                max_query_length=64,\n",
        "                is_training=False,\n",
        "                return_dataset=\"pt\",\n",
        "                threads=1,\n",
        "            )\n",
        "\n",
        "            eval_sampler = SequentialSampler(dataset)\n",
        "            eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=10)\n",
        "\n",
        "            all_results = []\n",
        "\n",
        "            for batch in eval_dataloader:\n",
        "                model.eval()\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    inputs = {\n",
        "                        \"input_ids\": batch[0],\n",
        "                        \"attention_mask\": batch[1],\n",
        "                        \"token_type_ids\": batch[2],\n",
        "                    }\n",
        "\n",
        "                    example_indices = batch[3]\n",
        "\n",
        "                    outputs = model(**inputs)\n",
        "                    print(outputs[0])\n",
        "                    for i, example_index in enumerate(example_indices):\n",
        "                        eval_feature = features[example_index.item()]\n",
        "                        unique_id = int(eval_feature.unique_id)\n",
        "\n",
        "                        output = [to_list(output[i]) for output in outputs]\n",
        "                      \n",
        "                        # print(len(output))\n",
        "                        start_logits, end_logits = output\n",
        "                        result = SquadResult(unique_id, start_logits, end_logits)\n",
        "                        all_results.append(result)\n",
        "            # print(len(all_results))\n",
        "            output_prediction_file = \"predictions.json\"\n",
        "            output_nbest_file = \"nbest_predictions.json\"\n",
        "            output_null_log_odds_file = \"null_predictions.json\"\n",
        "\n",
        "            predictions = compute_predictions_logits(\n",
        "                examples,\n",
        "                features,\n",
        "                all_results,\n",
        "                n_best_size,\n",
        "                max_answer_length,\n",
        "                do_lower_case,\n",
        "                output_prediction_file,\n",
        "                output_nbest_file,\n",
        "                output_null_log_odds_file,\n",
        "                False,  # verbose_logging\n",
        "                True,  # version_2_with_negative\n",
        "                null_score_diff_threshold,\n",
        "                tokenizer,\n",
        "            )\n",
        "\n",
        "            return predictions\n",
        "        \n",
        "\n",
        "        predictions = run_prediction(question_texts, context_text)\n",
        "\n",
        "# Print results\n",
        "        import statistics as st\n",
        "        prelist = [i for i in predictions.values() if i != '']\n",
        "        try:\n",
        "          return {\"prediction\": st.mode(prelist)}\n",
        "        except:\n",
        "          return {\"prediction\": prelist}\n",
        "        \n",
        "    except:\n",
        "        \n",
        "        return {\"prediction\": \"error\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9Irdv5S1Kfm"
      },
      "source": [
        "cc.run_app(app=app)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QM1YsaS1qql"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}